{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surrogate model (Kinematics to Muscle activation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 69\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "from utils import DATA_DIR\n",
    "from opencap_dataloader import OpenCapLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(os.path.join(DATA_DIR, 'Data')):\n",
    "    print(file)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/data/panini/HumanML3D/train/new_joints_vecs/\"\n",
    "label_path = \"/data/panini/HumanML3D/train/data/train/original_texts/\"\n",
    "\n",
    "action_to_desc = {\n",
    "        \"bend and pull full\" : 0,\n",
    "        \"countermovement jump\" : 1,\n",
    "        \"left countermovement jump\" : 2,\n",
    "        \"left lunge and twist\" : 3,\n",
    "        \"left lunge and twist full\" : 4,\n",
    "        \"right countermovement jump\" : 5,\n",
    "        \"right lunge and twist\" : 6,\n",
    "        \"right lunge and twist full\" : 7,\n",
    "        \"right single leg squat\" : 8,\n",
    "        \"squat\" : 9,\n",
    "        \"bend and pull\" : 10,\n",
    "        \"left single leg squat\" : 11,\n",
    "        \"push up\" : 12\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from collections import Counter\n",
    "import torch.optim as optim\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, action_to_desc, max_seq_len, stride, mode):\n",
    "        data_path = '/data/panini/HumanML3D/' + mode + \"/new_joints_vecs/\"\n",
    "        self.data_path_mot = '/data/panini/HumanML3D/' + \"/mot_data/\"\n",
    "        label_path = '/data/panini/HumanML3D/' + mode +  \"/original_texts/\"\n",
    "        # self.mcs_path = mode + \"/mcs/\"\n",
    "        self.data_path = data_path\n",
    "        self.label_path = label_path\n",
    "        self.action_to_desc = action_to_desc\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.stride = stride\n",
    "        self.samples = []\n",
    "        \n",
    "        class_counter = Counter()\n",
    "        \n",
    "        files = os.listdir(data_path)\n",
    "        print(\"Len num samples in file:\", len(files))\n",
    "        for file in files:\n",
    "            if not file.endswith('.npy'):\n",
    "                continue\n",
    "            # data_file = np.load(data_path + file)\n",
    "            txt_file = file.replace('.npy','.txt')\n",
    "            label_file = label_path + txt_file\n",
    "            # mcs_file = self.mcs_path + txt_file\n",
    "            # if not os.path.isfile(mcs_file):\n",
    "            #     mcs_score = -1\n",
    "            # else:\n",
    "            #     with open(mcs_file, 'r') as f:\n",
    "            #         mcs_score = f.readlines()\n",
    "            #         mcs_score = float(mcs_score[0])\n",
    "            with open(label_file, 'r') as f:\n",
    "                label_text = f.readlines()\n",
    "            label = action_to_desc[label_text[0]]\n",
    "            class_counter[label] += 1\n",
    "            self.samples.append((data_path + file, label))\n",
    "\n",
    "        # Calculate the maximum frequency of any class\n",
    "        max_class_frequency = max(class_counter.values())\n",
    "\n",
    "        # Calculate the sampling weight for each class\n",
    "        self.class_weights = {cls: max_class_frequency / freq for cls, freq in class_counter.items()}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            file, label = self.samples[idx]\n",
    "            data_file = np.load(file)\n",
    "            # mot_file = self.data_path_mot + file.split('/')[-1]\n",
    "            # mot_file = np.load(mot_file)\n",
    "            \n",
    "            # data_file = np.concatenate((data_file,mot_file[:-1]), axis = 1)\n",
    "            # print(data_file.shape)\n",
    "            \n",
    "            # Determine sampling probability based on class frequency\n",
    "            sampling_prob = 1 / self.class_weights[label]\n",
    "\n",
    "            # Perform sampling based on the calculated probability\n",
    "            if np.random.uniform() < sampling_prob:\n",
    "                if data_file.shape[0] <= self.max_seq_len*self.stride:\n",
    "                    padded_data = np.zeros((self.max_seq_len, data_file.shape[1]))\n",
    "                    temp = data_file[::self.stride,:]\n",
    "                    padded_data[:temp.shape[0], :] = temp\n",
    "                    data_file = padded_data\n",
    "                    start = 0\n",
    "                else:\n",
    "                    # print(len(data_file), max_seq_len)\n",
    "                    i = np.random.randint(0,len(data_file)-self.max_seq_len*self.stride)\n",
    "                    data_file = data_file[i:i+self.max_seq_len*self.stride:self.stride, :]\n",
    "                    start = i\n",
    "                # print(data_file.shape)\n",
    "                \n",
    "                # Only take every alternate time stamp\n",
    "                alternate_data = data_file # Take every alternate row\n",
    "                # print(alternate_data, label, start, file)\n",
    "                return alternate_data, label, start, file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 50\n",
    "batch_size = 512\n",
    "stride = 3\n",
    "\n",
    "train_dataset = CustomDataset(action_to_desc, max_seq_len, stride, mode=\"train\")\n",
    "train_dataset.samples = [sample for sample in train_dataset.samples if sample[0] is not None]\n",
    "test_dataset = CustomDataset(action_to_desc, max_seq_len, stride, mode=\"eval\")\n",
    "test_dataset.samples = [sample for sample in test_dataset.samples if sample[0] is not None]\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = max_seq_len * 263\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 256\n",
    "num_classes = 13 #len(np.unique(Y))  # Assuming Y contains integer labels\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc3_lab = nn.Linear(hidden_size1, num_classes)\n",
    "        # self.dp = nn.Dropout(0)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, return_embeddings = False):\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.relu(self.fc1(x)) \n",
    "        if return_embeddings:\n",
    "            return x\n",
    "        # x = self.relu(self.fc2(x)) + x\n",
    "        x_lab = self.fc3_lab(x)\n",
    "        # x_mcs = self.fc3_mcs(x)\n",
    "        return x_lab #, x_mcs\n",
    "\n",
    "# Create an instance of the MLP model\n",
    "model = MLP(input_size, hidden_size1, hidden_size2, num_classes).to('cuda')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "    return correct, total\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels, start, file in train_loader:\n",
    "        \n",
    "        inputs = inputs.float()  # Convert inputs to float\n",
    "        optimizer.zero_grad()\n",
    "        outputs_lab= model(inputs.to('cuda'))\n",
    "        \n",
    "        loss_lab = criterion(outputs_lab, labels.type(torch.long).to('cuda'))\n",
    "        loss = loss_lab.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "        model.eval()\n",
    "        correct_lab = 0\n",
    "        total_lab = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels, start, file in test_loader:\n",
    "                inputs = inputs.float()  # Convert inputs to float\n",
    "                labels = labels.to('cuda')\n",
    "                outputs_lab = model(inputs.to('cuda'))\n",
    "                # Calculate label accuracy\n",
    "                correct, total = calculate_accuracy(outputs_lab, labels)\n",
    "                correct_lab += correct\n",
    "                total_lab += total\n",
    "            # Print accuracy\n",
    "            print(f\"Label Accuracy on test set: {(correct_lab / total_lab) * 100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "torch.save(model.state_dict(), \"MCS_classifier_anwesh.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(dataloader, model):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    start = []\n",
    "    files = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, lab, start_point, file_name in dataloader:\n",
    "            inputs = inputs.float().to('cuda')\n",
    "            emb = model(inputs, return_embeddings=True)\n",
    "            embeddings.append(emb.cpu().numpy())\n",
    "            labels.append(lab.cpu().numpy())\n",
    "            # print(start_point)\n",
    "            start.append(start_point.cpu().numpy())\n",
    "            file_name = list(file_name)\n",
    "            # print(file_name)\n",
    "            files.append(file_name)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    labels = np.hstack(labels)\n",
    "    start = np.hstack(start)\n",
    "    files = np.hstack(files)\n",
    "    return embeddings, labels, start, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and store training embeddings\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "train_embeddings, train_labels, train_start_points, train_files = extract_embeddings(train_loader, model)\n",
    "\n",
    "# Extract and store test embeddings\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "test_embeddings, test_labels, test_start_points, test_files = extract_embeddings(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Function to plot embeddings\n",
    "def plot_embeddings(embeddings, labels, title):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for label in np.unique(labels):\n",
    "        indices = np.where(labels == label)\n",
    "        plt.scatter(reduced_embeddings[indices, 0], reduced_embeddings[indices, 1], label=label, alpha=0.5)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('t-SNE component 1')\n",
    "    plt.ylabel('t-SNE component 2')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize training embeddings\n",
    "plot_embeddings(train_embeddings, train_labels, 'Training Embeddings Clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Step 1: Combine Train and Test Data\n",
    "combined_embeddings = np.vstack((train_embeddings, test_embeddings))\n",
    "combined_labels = np.hstack((train_labels, test_labels))\n",
    "combined_is_train = np.hstack((np.ones(len(train_labels)), np.zeros(len(test_labels))))\n",
    "comined_names = np.hstack((train_files, test_files))\n",
    "\n",
    "# Step 2: Calculate t-SNE embeddings for the combined data\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "combined_tsne_embeddings = tsne.fit_transform(combined_embeddings)\n",
    "\n",
    "# Step 3: Separate the transformed data back into train and test sets\n",
    "train_tsne_embeddings = combined_tsne_embeddings[combined_is_train == 1]\n",
    "test_tsne_embeddings = combined_tsne_embeddings[combined_is_train == 0]\n",
    "\n",
    "# Step 4: Identify and remove outliers in the t-SNE space\n",
    "def get_mean_embeddings_in_tsne_space(tsne_embeddings, labels):\n",
    "    mean_embeddings = {}\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    for label in unique_labels:\n",
    "        # Get t-SNE embeddings for the current label\n",
    "        label_embeddings = tsne_embeddings[labels == label]\n",
    "        \n",
    "        # Use EllipticEnvelope to identify and remove outliers\n",
    "        ee = EllipticEnvelope(contamination=0.1)  # Adjust contamination to control the percentage of outliers\n",
    "        ee.fit(label_embeddings)\n",
    "        inliers = ee.predict(label_embeddings) == 1\n",
    "        \n",
    "        # Filter out the outliers\n",
    "        filtered_embeddings = label_embeddings[inliers]\n",
    "        \n",
    "        # Calculate the mean of the remaining points\n",
    "        mean_of_best_samples = np.mean(filtered_embeddings, axis=0)\n",
    "        \n",
    "        # Store the result\n",
    "        mean_embeddings[label] = mean_of_best_samples\n",
    "    \n",
    "    return mean_embeddings\n",
    "\n",
    "# Calculate mean embeddings in t-SNE space for the training data\n",
    "mean_tsne_embeddings = get_mean_embeddings_in_tsne_space(train_tsne_embeddings, train_labels)\n",
    "\n",
    "# Step 5: Plot the combined t-SNE embeddings with mean points\n",
    "def plot_combined_tsne_with_means(train_tsne_embeddings, train_labels, test_tsne_embeddings, test_labels, mean_tsne_embeddings, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot train data\n",
    "    for label in np.unique(train_labels):\n",
    "        indices = np.where(train_labels == label)\n",
    "        plt.scatter(train_tsne_embeddings[indices, 0], train_tsne_embeddings[indices, 1], label=f'Train Class {label}', alpha=0.5)\n",
    "    \n",
    "    # Plot test data\n",
    "    for label in np.unique(test_labels):\n",
    "        indices = np.where(test_labels == label)\n",
    "        plt.scatter(test_tsne_embeddings[indices, 0], test_tsne_embeddings[indices, 1], label=f'Test Class {label}', alpha=0.5, marker='x')\n",
    "    \n",
    "    # Plot mean points\n",
    "    for label, mean_embedding in mean_tsne_embeddings.items():\n",
    "        plt.scatter(mean_embedding[0], mean_embedding[1], color='black', marker='o', s=100, label=f'Mean Train Class {label}')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('t-SNE component 1')\n",
    "    plt.ylabel('t-SNE component 2')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the combined t-SNE embeddings with mean points\n",
    "plot_combined_tsne_with_means(train_tsne_embeddings, train_labels, test_tsne_embeddings, test_labels, mean_tsne_embeddings, 'Combined t-SNE Embeddings with Mean Points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Calculate the distances for train and test embeddings from their respective mean points\n",
    "def calculate_distances_to_means(tsne_embeddings, labels, mean_tsne_embeddings):\n",
    "    distances = np.zeros(len(labels))\n",
    "    for i, (embedding, label) in enumerate(zip(tsne_embeddings, labels)):\n",
    "        mean_embedding = mean_tsne_embeddings[label]\n",
    "        distance = np.linalg.norm(embedding - mean_embedding)\n",
    "        distances[i] = distance\n",
    "    return distances\n",
    "\n",
    "# Calculate distances for train and test embeddings\n",
    "train_distances = calculate_distances_to_means(train_tsne_embeddings, train_labels, mean_tsne_embeddings)\n",
    "test_distances = calculate_distances_to_means(test_tsne_embeddings, test_labels, mean_tsne_embeddings)\n",
    "\n",
    "# Step 2: Visualize the distances for the train data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_distances, bins=50, alpha=0.75, color='blue', edgecolor='black')\n",
    "plt.title('Histogram of Distances from Mean Embeddings (Train Data)')\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Assign scores based on distance thresholds (optional)\n",
    "# You can determine the thresholds visually from the histogram and then use them to assign scores\n",
    "# For example:\n",
    "# thresholds = [threshold1, threshold2, threshold3, threshold4]\n",
    "# scores = np.digitize(train_distances, bins=thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the thresholds\n",
    "thresholds = [2, 4, 8, 10]\n",
    "\n",
    "# Step 2: Function to assign scores based on thresholds\n",
    "def assign_scores(distances, thresholds):\n",
    "    scores = np.zeros(len(distances), dtype=int)\n",
    "    for i, distance in enumerate(distances):\n",
    "        if distance < thresholds[0]:\n",
    "            scores[i] = 5\n",
    "        elif distance < thresholds[1]:\n",
    "            scores[i] = 4\n",
    "        elif distance < thresholds[2]:\n",
    "            scores[i] = 3\n",
    "        elif distance < thresholds[3]:\n",
    "            scores[i] = 2\n",
    "        else:\n",
    "            scores[i] = 1\n",
    "    return scores\n",
    "\n",
    "# Step 3: Assign scores to train and test distances\n",
    "train_scores = assign_scores(train_distances, thresholds)\n",
    "test_scores = assign_scores(test_distances, thresholds)\n",
    "\n",
    "# Print the results\n",
    "print(\"Train Scores:\", train_scores)\n",
    "print(\"Test Scores:\", test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_scores = np.hstack((train_scores, test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, score in zip(comined_names,combined_scores):\n",
    "    mode = name.split('/')[0]\n",
    "    fname = name.split('/')[-1].split('.')[0]\n",
    "    save_path = mode + \"/new_score/\" + fname + \".txt\"\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.write(str(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
